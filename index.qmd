---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
number-sections: true
format:
  html: default
  pdf: default
execute:
  echo: true
  eval: true
---

See [Discussion Questions](#sec-discussion) to review the analysis answers.


# üå≥ Decision Tree Challenge - Feature Importance and Variable Encoding

## Challenge Overview

**Your Mission:** Create a simple GitHub Pages site that demonstrates how decision trees measure feature importance and analyzes the critical differences between categorical and numerical variable encoding. You'll answer two key discussion questions by adding narrative to a pre-built analysis and posting those answers to your GitHub Pages site as a rendered HTML document.

::: {.callout-warning}
## ‚ö†Ô∏è AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance ‚Üí Awareness ‚Üí Learning ‚Üí Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## The Decision Tree Problem üéØ

> "The most important thing in communication is hearing what isn't said." - Peter Drucker

**The Core Problem:** Decision trees are often praised for their interpretability and ability to handle both numerical and categorical variables. But what happens when we encode categorical variables as numbers? How does this affect our understanding of feature importance?

**What is Feature Importance?** In decision trees, feature importance measures how much each variable contributes to reducing impurity (or improving prediction accuracy) across all splits in the tree. It's a key metric for understanding which variables matter most for your predictions.

::: {.callout-important}
## üéØ The Key Insight: Encoding Matters for Interpretability

**The problem:** When we encode categorical variables as numerical values (like 1, 2, 3, 4...), decision trees treat them as if they have a meaningful numerical order. This can completely distort our analysis.

**The Real-World Context:** In real estate, we know that neighborhood quality, house style, and other categorical factors are crucial for predicting home prices. But if we encode these as numbers, we might get misleading insights about which features actually matter most.

**The Devastating Reality:** Even sophisticated machine learning models can give us completely wrong insights about feature importance if we don't properly encode our variables. A categorical variable that should be among the most important might appear irrelevant, while a numerical variable might appear artificially important.

:::

Let's assume we want to predict house prices and understand which features matter most. The key question is: **How does encoding categorical variables as numbers affect our understanding of feature importance?**

## The Ames Housing Dataset üè†

We are analyzing the Ames Housing dataset which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is perfect for our analysis because it contains a categorical variable (like zip code) and numerical variables (like square footage, year built, number of bedrooms).

## The Problem: ZipCode as Numerical vs Categorical

**Key Question:** What happens when we treat zipCode as a numerical variable in a decision tree? How does this affect feature importance interpretation?

**The Issue:** Zip codes (50010, 50011, 50012, 50013) are categorical variables representing discrete geographic areas, i.e. neighborhoods. When treated as numerical, the tree might split on "zipCode > 50012.5" - which has no meaningful interpretation for house prices.  Zip codes are non-ordinal categorical variables meaning they have no inherent order that aids house price prediction (i.e. zip code 99999 is not the priceiest zip code).

## Data Loading and Model Building

::: {.callout-important}
## üéØ Note on Python Usage

You have not been coached through setting up a Python environment.  You will need to set up a Python environment and install the necessary packages to run this code - takes about 15 minutes; see [https://quarto.org/docs/projects/virtual-environments.html](https://quarto.org/docs/projects/virtual-environments.html).  Alternatively, delete the Python code and only leave the remaining R code that is provided.  You can see the executed Python output at my GitHub pages site: [https://flyaflya.github.io/decTreeChallenge/](https://flyaflya.github.io/decTreeChallenge/).

:::

::: {.panel-tabset}

@sec-discussion 
*Remember: Frequent commits are your safety net!*
:::

## Discussion Questions {#sec-discussion}

**Your Task:** Add thoughtful narrative answers to these two questions in the Discussion Questions section of your rendered HTML site.
## Discussion Questions {#sec-discussion}



1. **Numerical vs Categorical Encoding:** There are four models above, two in R and two in Python. For each language, the models differ by how zip code is modelled, either as a numerical variable or as a categorical variable. Given what you know about zip codes and real estate prices, how should zip code be modelled, numerically or categorically?

I think Zip Code should be modeled categorically, not numerically. Treating it as a number creates fake order (e.g., ‚Äú50013 > 50012‚Äù) and leads trees to make threshold splits that have no geographic meaning, which can distort feature importance. In R, the rpart package handles this correctly by treating zip codes as factors, allowing the model to split by meaningful groups of areas rather than numeric thresholds. In Python, the DecisionTreeRegressor does not natively handle categorical data, so if zip codes are entered as numbers, the model will misinterpret them as ordered values. To address this, one-hot encoding should be used so that each zip code is treated as its own category. In short, zip codes should be modeled as categorical variables, with factors in R and one-hot encoded features in Python, so the model can correctly capture neighborhood patterns and provide accurate, interpretable results. 


2. **R vs Python Implementation Differences:** When modelling zip code as a categorical variable, the output tree and feature importance differs quite significantly between R and Python. Investigate why this is the case. Which language would you say does a better job of modelling zip code as a categorical variable? Why is this the case? Do you see any documentation suggesting the other language does a better job? If so, please provide a quote from the documentation.

R and Python handle categorical data differently, which explains their contrasting decision tree results. In R, the rpart function naturally supports categorical variables. When zip code is defined as a factor, R automatically groups categories to create meaningful splits, such as separating certain neighborhoods from others. This makes the model‚Äôs output easier to interpret.

Python‚Äôs DecisionTreeRegressor, however, cannot process categorical data directly. Zip codes must be converted using one-hot encoding, which creates separate columns for each value. This spreads the importance of location across multiple columns, making the results harder to interpret.

According to scikit-learn documentation, ‚ÄúDecision trees do not support categorical variables directly. Categorical variables need to be converted to numerical form (for example, by using one-hot encoding).‚Äù In short, R handles categorical data more naturally and produces a clearer, more interpretable model than Python.



::: {.callout-important}
## üìã Important Note on AI Usage

**No coding assistance expected:** This challenge is designed for independent analysis and critical thinking. You are **not** expected to use AI for coding help or to write code for you. You are **not** expected to code at all unless curious about any ideas you may have. 

**AI as unreliable thought partner:** If you choose to use AI tools, treat them as an unreliable thought partner. AI responses should be verified and cross-checked rather than accepted at face value.

**Documentation verification required:** For question 2, you must investigate the official documentation for both `rpart` (R) and `sklearn.tree.DecisionTreeRegressor` (Python) to understand why the two implementations yield vastly different results when handling categorical variables. Your analysis should be grounded in the actual documentation and technical specifications of these libraries, not AI-generated explanations. AI can be helpful in digesting the documentation, but you must verify the information is correct.
:::

## Grading Rubric üéì

::: {.callout-important}
## üìä What You're Really Being Graded On

**This is an investigative report, not a coding exercise.** You're analyzing decision tree models and reporting your findings like a professional analyst would. Think of this as a brief you'd write for a client or manager about why proper variable encoding matters in machine learning.

**What makes a great report:**

- **Clear narrative:** Tell the story of what you discovered about decision tree feature importance
- **Insightful analysis:** Focus on the most interesting differences between numerical and categorical encoding
- **Professional presentation:** Clean, readable, and engaging
- **Concise conclusions:** No AI babble or unnecessary technical jargon
- **Human insights:** Your interpretation of what the feature importance rankings actually mean (or don't mean)
- **Documentation-based analysis:** For question 2, ground your analysis in actual library documentation

**What we're looking for:** A compelling 1-2 minute read that demonstrates both the power of decision trees for interpretability and the critical importance of proper variable encoding.
:::

### Questions to Answer for 75% Grade on Challenge

1. **Numerical vs Categorical Analysis:** Provide a clear, well-reasoned answer to question 1 about how zip codes should be modelled. Your answer should demonstrate understanding of why categorical variables need special treatment in decision trees.

### Questions to Answer for 85% Grade on Challenge

2. **R vs Python Implementation Analysis:** Provide a thorough analysis of question 2, including investigation of the official documentation for both `rpart` (R) and `sklearn.tree.DecisionTreeRegressor` (Python). Your analysis should explain the technical differences and provide a reasoned opinion about which implementation handles categorical variables better.

### Questions to Answer for 95% Grade on Challenge

3. **Professional Presentation:** Your discussion answers should be written in a professional, engaging style that would be appropriate for a business audience. Avoid technical jargon and focus on practical implications.  Use Quarto markdown linking to create a link to the discussion section from the top of the page (see https://quarto.org/docs/authoring/cross-references.html#sections).

### Questions to Answer for 100% Grade on Challenge

4. **Documentation Integration:** For question 2, include a specific quote from the official documentation of `sklearn.tree.DecisionTreeRegressor` that supports your analysis. 

## Submission Checklist ‚úÖ

**Minimum Requirements (Required for Any Points):**

- [ ] Forked starter repository from [https://github.com/flyaflya/decTreeChallenge.git](https://github.com/flyaflya/decTreeChallenge.git)
- [ ] Cloned repository locally using Cursor (or VS Code)
- [ ] Added thoughtful narrative answers to both discussion questions
- [ ] Document rendered to HTML successfully
- [ ] HTML files uploaded to your forked repository
- [ ] GitHub Pages enabled and working
- [ ] Site accessible at `https://[your-username].github.io/decTreeChallenge/`

**75% Grade Requirements:**

- [ ] Clear, well-reasoned answer to question 1 about numerical vs categorical encoding

**85% Grade Requirements:**

- [ ] Thorough analysis of question 2 with investigation of official documentation

**95% Grade Requirements:**

- [ ] Professional presentation style appropriate for business audience with links to the discussion section from the top of the page (see https://quarto.org/docs/authoring/cross-references.html#sections).

**100% Grade Requirements:**

- [ ] Specific quote from official documentation of `sklearn.tree.DecisionTreeRegressor` supporting your analysis

**Report Quality (Critical for Higher Grades):**

- [ ] Clear, engaging narrative that tells a story
- [ ] Focus on the most interesting findings about decision tree feature importance
- [ ] Professional writing style (no AI-generated fluff)
- [ ] Concise analysis that gets to the point
- [ ] Practical insights that would help a real data scientist
- [ ] Documentation-based analysis for technical questions

